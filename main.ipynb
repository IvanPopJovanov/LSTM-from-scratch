{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQCWb4swT9Er",
        "outputId": "edc5d2e1-376c-4eee-f218-01df29428fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxvnhDNLUT2V",
        "outputId": "a274d08b-c902-48d0-f574-2061085d25c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ri-projekat\n",
            "bible.txt  main.ipynb         \u001b[0m\u001b[01;34mmodel-1661265670\u001b[0m/  \u001b[01;34mmodel-1661294627\u001b[0m/\n",
            "input.txt  \u001b[01;34mmodel-1660904954\u001b[0m/  \u001b[01;34mmodel-1661265700\u001b[0m/  test.pickle\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/ri-projekat\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KhEH2Jvv57OQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.utils.extmath import cartesian\n",
        "import pickle\n",
        "import time\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('error', RuntimeWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H59xiWYYQwbQ"
      },
      "outputs": [],
      "source": [
        "def saveModel(model,filename):\n",
        "  with open(filename, \"wb\") as outfile:\n",
        "    pickle.dump(model, outfile)\n",
        "  pass\n",
        "\n",
        "def loadModel(filename):\n",
        "  with open(filename, \"rb\") as infile:\n",
        "    model = pickle.load(infile)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q2D7iZUqRX3p"
      },
      "outputs": [],
      "source": [
        "class ActivationLinear:\n",
        "  def forward(self,X):\n",
        "    return X\n",
        "  def backward(self,dh,output):\n",
        "    return dh\n",
        "\n",
        "class ActivationReLU:\n",
        "  def forward(self,X):\n",
        "    return np.maximum(0,X)\n",
        "  def backward(self,dh,output):\n",
        "    dh[output <= 0] = 0\n",
        "    return dh\n",
        "\n",
        "class ActivationTanh:\n",
        "  def forward(self,X):\n",
        "    return np.tanh(X)\n",
        "  def backward(self,dh,output):\n",
        "    dh = (1 - output*output) * dh\n",
        "    return dh\n",
        "\n",
        "class ActivationSigmoid:\n",
        "  def forward(self,X):\n",
        "    return 1/(1+np.exp(-X))\n",
        "  def backward(self,dh,output):\n",
        "    dh = output*(1-output) * dh\n",
        "    return dh\n",
        "\n",
        "class ActivationSoftmax:\n",
        "  def forward(self,X):\n",
        "    eX = np.exp(X)\n",
        "    return eX / np.sum(eX, axis=1, keepdims=True)\n",
        "  def backward(self,dh,output):\n",
        "    return dh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VNvz377F9JhI"
      },
      "outputs": [],
      "source": [
        "class Dense:\n",
        "  def __init__(self,input_size,layer_size,activation=ActivationLinear()):\n",
        "    self.input_size = input_size\n",
        "    self.layer_size = layer_size\n",
        "    self.W = 0.1*np.random.randn(input_size,layer_size)\n",
        "    self.b = 0.1*np.random.randn(1,layer_size)\n",
        "    self.activation = activation\n",
        "  def reset(self):\n",
        "    pass\n",
        "  def forward(self,X):\n",
        "    # self.W = np.clip(self.W,-1,1)\n",
        "    # self.b = np.clip(self.b,-1,1)\n",
        "    self.input = X\n",
        "    val = np.dot(X,self.W) + self.b\n",
        "    self.output = self.activation.forward(val)\n",
        "    return self.output\n",
        "  def backward(self,d):\n",
        "    d = self.activation.backward(d,self.output)\n",
        "    dW = np.dot(self.input.T,d)\n",
        "    db = np.sum(d,axis=0,keepdims=True)\n",
        "    dh = np.dot(d,self.W.T)\n",
        "\n",
        "    dW = np.clip(dW,-1,1)\n",
        "    db = np.clip(db,-1,1)\n",
        "\n",
        "    return dh, dW, db, dW\n",
        "\n",
        "class Recurrent:\n",
        "  def __init__(self,input_size,layer_size,activation=ActivationLinear()):\n",
        "    self.input_size = input_size\n",
        "    self.layer_size = layer_size\n",
        "    self.W = 0.1*np.random.randn(input_size,layer_size)\n",
        "    self.Wh = 0.1*np.random.randn(layer_size,layer_size)\n",
        "    self.b = 0.1*np.random.randn(1,layer_size)\n",
        "    self.hprev = np.zeros((1,self.layer_size))\n",
        "    # self.h = np.zeros((1,layer_size))\n",
        "    self.activation = activation\n",
        "  def reset(self):\n",
        "    self.hprev = np.zeros((1,self.layer_size))\n",
        "  def forward(self,X):\n",
        "    # self.W = np.clip(self.W,-1,1)\n",
        "    # self.b = np.clip(self.b,-1,1)\n",
        "    # self.Wh = np.clip(self.Wh,-1,1)\n",
        "    self.input = X\n",
        "    self.output = np.zeros((X.shape[0],self.layer_size))\n",
        "    self.h = np.zeros((X.shape[0],self.layer_size))\n",
        "    self.h[0] = self.hprev\n",
        "    for i in range(X.shape[0]):\n",
        "      val = np.dot(X[i],self.W) + np.dot(self.h[i], self.Wh) + self.b\n",
        "      self.output[i] = self.activation.forward(val)\n",
        "      if(i+1 < X.shape[0]):\n",
        "        self.h[i+1] = self.output[i]\n",
        "      else:\n",
        "        self.hprev = self.output[i]\n",
        "    return self.output\n",
        "  def backward(self,d):\n",
        "    dW = np.zeros_like(self.W)\n",
        "    db = np.zeros_like(self.b)\n",
        "    dWh = np.zeros_like(self.Wh)\n",
        "    for i in reversed(range(d.shape[0])):\n",
        "      d[i] = self.activation.backward(d[i],self.output[i])\n",
        "      dW += np.dot(self.input[i][np.newaxis].T,d[i][np.newaxis])\n",
        "      db += np.sum(d[i],axis=0,keepdims=True)\n",
        "      dWh += np.dot(self.h[i][np.newaxis].T,d[i][np.newaxis])\n",
        "    dh = np.dot(d,self.W.T)\n",
        "\n",
        "    dW = np.clip(dW,-1,1)\n",
        "    db = np.clip(db,-1,1)\n",
        "    dWh = np.clip(dWh,-1,1)\n",
        "\n",
        "    return dh, dW, db, dWh\n",
        "\n",
        "class LSTM:\n",
        "  def __init__(self,input_size,layer_size,activation=ActivationLinear()):\n",
        "    self.input_size = input_size\n",
        "    self.layer_size = layer_size\n",
        "    self.W = 0.1*np.random.randn(input_size + layer_size,4*layer_size)\n",
        "    self.b = 0.1*np.random.randn(1,4*layer_size)\n",
        "    self.hprev = np.zeros((1,self.layer_size))\n",
        "    self.cprev = np.zeros((1,self.layer_size))\n",
        "    self.activation = activation\n",
        "    self.tanh = ActivationTanh()\n",
        "    self.sigmoid = ActivationSigmoid()\n",
        "  def reset(self):\n",
        "    self.hprev = np.zeros((1,self.layer_size))\n",
        "    self.cprev = np.zeros((1,self.layer_size))\n",
        "  def forward(self,X):\n",
        "    self.input = X\n",
        "    self.output = np.zeros((X.shape[0],self.layer_size))\n",
        "    self.h = np.zeros((X.shape[0],self.input_size + self.layer_size))\n",
        "    self.h[0,self.input_size:] = self.hprev\n",
        "    self.c = np.zeros((X.shape[0],self.layer_size))\n",
        "    self.c[0] = self.cprev\n",
        "    self.ct = np.zeros((X.shape[0],self.layer_size))\n",
        "    self.IFOG = np.zeros((X.shape[0],self.W.shape[1]))\n",
        "    self.IFOGf = np.zeros((X.shape[0],self.W.shape[1]))\n",
        "    for i in range(X.shape[0]):\n",
        "      self.h[i,0:self.input_size] = np.copy(X[i])\n",
        "      self.IFOG[i] = np.dot(self.h[i],self.W) + self.b\n",
        "      self.IFOGf[i,:self.layer_size] = self.tanh.forward(self.IFOG[i,:self.layer_size])\n",
        "      self.IFOGf[i,self.layer_size:] = self.sigmoid.forward(self.IFOG[i,self.layer_size:])\n",
        "      ctmp = self.IFOGf[i,:self.layer_size] * self.IFOGf[i,self.layer_size:2*self.layer_size]\n",
        "      ctmp += self.IFOGf[i,2*self.layer_size:3*self.layer_size] * self.c[i]\n",
        "      self.ct[i] = self.tanh.forward(ctmp)\n",
        "      htmp = self.IFOGf[i,3*self.layer_size:] * self.ct[i]\n",
        "      if(i+1 < X.shape[0]):\n",
        "        self.c[i+1] = ctmp\n",
        "        self.h[i+1,self.input_size:] = htmp        \n",
        "      else:\n",
        "        self.cprev = ctmp\n",
        "        self.hprev = htmp\n",
        "\n",
        "      val = htmp\n",
        "      self.output[i] = self.activation.forward(val)\n",
        "    return self.output\n",
        "  def backward(self,d):\n",
        "    dW = np.zeros_like(self.W)\n",
        "    db = np.zeros_like(self.b)\n",
        "\n",
        "    dIFOG = np.zeros(self.IFOG.shape)\n",
        "    dIFOGf = np.zeros(self.IFOGf.shape)\n",
        "    dc = np.zeros(self.c.shape)\n",
        "\n",
        "    d = self.activation.backward(d,self.output)\n",
        "\n",
        "    for i in reversed(range(d.shape[0])):\n",
        "\n",
        "      dIFOGf[i,3*self.layer_size:] = self.ct[i] * d[i]\n",
        "\n",
        "      dc[i] += self.tanh.backward(self.IFOGf[i,3*self.layer_size:]*d[i],self.ct[i])\n",
        "\n",
        "      dIFOGf[i,:self.layer_size] = self.IFOGf[i,self.layer_size:2*self.layer_size] * dc[i]\n",
        "      dIFOGf[i,self.layer_size:2*self.layer_size] = self.IFOGf[i,:self.layer_size] * dc[i]\n",
        "\n",
        "      dIFOGf[i,2*self.layer_size:3*self.layer_size] = self.c[i] * dc[i]\n",
        "      if(i>0):\n",
        "        dc[i-1]+= self.IFOGf[i,2*self.layer_size:3*self.layer_size] * dc[i]\n",
        "\n",
        "      dIFOG[i,:self.layer_size] = self.tanh.backward(dIFOGf[i,:self.layer_size],self.IFOGf[i,:self.layer_size])\n",
        "      dIFOG[i,self.layer_size:] = self.sigmoid.backward(dIFOGf[i,self.layer_size:],self.IFOGf[i,self.layer_size:])\n",
        "\n",
        "      dW += np.dot(self.h[i][np.newaxis].T,dIFOG[i][np.newaxis])\n",
        "      db += np.sum(dIFOG[i],axis=0,keepdims=True)\n",
        "\n",
        "      dh = np.dot(dIFOG[i],self.W.T)\n",
        "      if(i>0):\n",
        "        d[i-1] += dh[self.input_size:]\n",
        "\n",
        "    dh = np.dot(dIFOG[0],self.W.T)\n",
        "\n",
        "    dW = np.clip(dW,-1,1)\n",
        "    db = np.clip(db,-1,1)\n",
        "\n",
        "    return dh, dW, db, dW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vhgdHiq3veUc"
      },
      "outputs": [],
      "source": [
        "class LossMSE:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def calculate(self,y_pred,y_true):\n",
        "    return np.mean((y_pred - y_true)**2)\n",
        "  def derivative(self,y_pred,y_true):\n",
        "    return 2*(y_pred - y_true)/y_true.shape[0] \n",
        "\n",
        "class LossCrossEntropy:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def calculate(self,y_pred,y_true):\n",
        "    log_prob = -np.log(y_pred[range(y_true.shape[0]),y_true])\n",
        "    return np.mean(log_prob)\n",
        "  def derivative(self,y_pred,y_true):\n",
        "    d = np.copy(y_pred)\n",
        "    d[range(y_true.shape[0]),y_true] -= 1\n",
        "    return d/y_true.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r2QYzabVIqj0"
      },
      "outputs": [],
      "source": [
        "class OptimizerGD:\n",
        "  def __init__(self,n_epochs=10000,learning_rate=0.05,batch_size=0,loss=LossMSE(),displayProgress=None, printEvery = 100):\n",
        "    self.n_epochs = n_epochs\n",
        "    self.learning_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.loss_function = loss\n",
        "    self.displayProgress=displayProgress\n",
        "    self.printEvery = printEvery\n",
        "  def optimize(self,X,y,layers):\n",
        "    batch_size = self.batch_size\n",
        "    if batch_size == 0:\n",
        "      batch_size = X.shape[0]\n",
        "    for i in range(self.n_epochs+1):\n",
        "      for j in range(int(np.ceil(X.shape[0]/batch_size))):\n",
        "        batch_start = (j*batch_size) % X.shape[0]\n",
        "        batch_end = np.minimum(batch_start + batch_size, X.shape[0])\n",
        "        output = X[batch_start:batch_end+1]\n",
        "        for layer in layers:\n",
        "          output = layer.forward(output)\n",
        "\n",
        "        yt = y[batch_start:batch_end+1]\n",
        "\n",
        "        loss = self.loss_function.calculate(output,yt)\n",
        "        if(i % self.printEvery == 0 and j==0):\n",
        "          print('epoch %d/%d, loss: %f' % (i, self.n_epochs, loss))\n",
        "\n",
        "        dh = self.loss_function.derivative(output,yt)\n",
        "        for layer in reversed(layers):\n",
        "          dh, dW, db, dWh = layer.backward(dh)\n",
        "          layer.W -= self.learning_rate*dW\n",
        "          layer.b -= self.learning_rate*db\n",
        "\n",
        "          if hasattr(layer, 'Wh'):\n",
        "            layer.Wh -= self.learning_rate*dWh\n",
        "\n",
        "class OptimizerAdam:\n",
        "  def __init__(self,n_epochs=10000,learning_rate=0.05,batch_size=0,beta1=0.9,beta2=0.999,delta=1e-7,loss=LossMSE(),displayProgress=None, printEvery = 100):\n",
        "    self.n_epochs = n_epochs\n",
        "    self.learning_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.alpha = learning_rate\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.delta = delta\n",
        "    self.loss_function = loss\n",
        "    self.displayProgress=displayProgress\n",
        "    self.printEvery = printEvery\n",
        "    self.layersInitialized = False\n",
        "  def optimize(self,X,y,layers):\n",
        "    if not self.layersInitialized:\n",
        "      for layer in layers:\n",
        "        layer.mW = np.zeros_like(layer.W)\n",
        "        layer.vW = np.zeros_like(layer.W)\n",
        "        layer.mb = np.zeros_like(layer.b)\n",
        "        layer.vb = np.zeros_like(layer.b)\n",
        "        if hasattr(layer, 'Wh'):\n",
        "          layer.mWh = np.zeros_like(layer.Wh)\n",
        "          layer.vWh = np.zeros_like(layer.Wh)\n",
        "      if self.batch_size == 0:\n",
        "        self.batch_size = X.shape[0]\n",
        "\n",
        "      if isinstance(self.loss_function, LossCrossEntropy):\n",
        "        self.smooth_loss = -np.log(1.0/X.shape[1])*self.batch_size\n",
        "      else:\n",
        "        self.smooth_loss = 0\n",
        "      self.i = 0\n",
        "      self.j = 0\n",
        "      self.layersInitialized = True\n",
        "\n",
        "    batch_size = self.batch_size\n",
        "\n",
        "    while self.i < self.n_epochs+1:\n",
        "      # for j in range(int(np.ceil(X.shape[0]/batch_size))):\n",
        "      #   batch_start = (j*batch_size) % X.shape[0]\n",
        "      #   batch_end = np.minimum(batch_start + batch_size, X.shape[0])\n",
        "      #   output = X[batch_start:batch_end+1]\n",
        "      while self.j < X.shape[0] - batch_size + 1:\n",
        "        batch_start = self.j\n",
        "        batch_end = self.j + batch_size\n",
        "        output = np.copy(X[batch_start:batch_end])\n",
        "\n",
        "        for layer in layers:\n",
        "          layer.reset()\n",
        "          output = layer.forward(output)\n",
        "\n",
        "        yt = np.copy(y[batch_start:batch_end])\n",
        "\n",
        "        loss = self.loss_function.calculate(output,yt)\n",
        "        if isinstance(self.loss_function, LossCrossEntropy):\n",
        "          self.smooth_loss = self.smooth_loss * 0.999 + loss * 0.001\n",
        "        else:\n",
        "          self.smooth_loss = loss\n",
        "        if(self.i % self.printEvery == 0 and self.j==0):\n",
        "          print('epoch %d/%d, loss: %f' % (self.i, self.n_epochs, self.smooth_loss))\n",
        "\n",
        "        dh = self.loss_function.derivative(output,yt)\n",
        "        for layer in reversed(layers):\n",
        "          dh, dW, db, dWh = layer.backward(dh)\n",
        "\n",
        "          layer.mW = self.beta1 * layer.mW + (1-self.beta1) * dW\n",
        "          layer.vW = self.beta2 * layer.vW + (1-self.beta2) * np.square(dW)\n",
        "          mW_hat = layer.mW / (1 - self.beta1**(self.i+1))\n",
        "          vW_hat = layer.vW / (1 - self.beta2**(self.i+1))\n",
        "          layer.W -= self.learning_rate*mW_hat/(np.sqrt(vW_hat) + self.delta)\n",
        "          \n",
        "          layer.mb = self.beta1 * layer.mb + (1-self.beta1) * db\n",
        "          layer.vb = self.beta2 * layer.vb + (1-self.beta2) * np.square(db)\n",
        "          mb_hat = layer.mb / (1 - self.beta1**(self.i+1))\n",
        "          vb_hat = layer.vb / (1 - self.beta2**(self.i+1))\n",
        "          layer.b -= self.learning_rate*db\n",
        "\n",
        "          if hasattr(layer, 'Wh'):\n",
        "            layer.mWh = self.beta1 * layer.mWh + (1-self.beta1) * dWh\n",
        "            layer.vWh = self.beta2 * layer.vWh + (1-self.beta2) * np.square(dWh)\n",
        "            mWh_hat = layer.mWh / (1 - self.beta1**(self.i+1))\n",
        "            vWh_hat = layer.vWh / (1 - self.beta2**(self.i+1))\n",
        "            layer.Wh -= self.learning_rate*dWh\n",
        "        self.j += 1\n",
        "      self.j = 0\n",
        "      if(self.i % self.printEvery == 0 and self.displayProgress):\n",
        "         self.displayProgress(self.model)\n",
        "      for layer in layers:\n",
        "        layer.reset()\n",
        "      self.i += 1\n",
        "      if(self.i % self.printEvery == 0):\n",
        "        saveModel(self.model, \"model-\" + str(self.model.timestamp) + \"/\" + \"checkpoint-\" + str(self.i) + \"-\" + str(round(self.smooth_loss,6)) + \".pickle\")\n",
        "    self.i = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fXGo2loc9ysc"
      },
      "outputs": [],
      "source": [
        "class Sequential:\n",
        "  def __init__(self,optimizer=OptimizerGD()):\n",
        "    self.timestamp = round(time.time())\n",
        "    os.makedirs(\"model-\" + str(self.timestamp), exist_ok=True)\n",
        "    self.layers = []\n",
        "    self.optimizer = optimizer\n",
        "    optimizer.model = self\n",
        "    print(\"Model \" + str(self.timestamp) + \" initialized.\")\n",
        "  def add(self,layer):\n",
        "    self.layers.append(layer)\n",
        "  def train(self,X,y):\n",
        "    self.optimizer.optimize(X,y,self.layers)\n",
        "  def predict(self,X):\n",
        "    output = np.copy(X)\n",
        "    for layer in self.layers:\n",
        "      layer.reset()\n",
        "      output = layer.forward(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FT7RB4NC_E5A"
      },
      "outputs": [],
      "source": [
        "def TestRegression():\n",
        "  X = np.arange(-1,1,0.01).reshape((-1,1))\n",
        "  y = np.sin(X*3*np.pi)\n",
        "\n",
        "  optimizer = OptimizerAdam(learning_rate=0.05,n_epochs=100000,batch_size = 0,loss=LossMSE(),printEvery=10000)\n",
        "\n",
        "  model = Sequential(optimizer)\n",
        "  model.add(Dense(1,10,activation = ActivationTanh()))\n",
        "  model.add(Dense(10,1,activation = ActivationLinear()))\n",
        "\n",
        "  model.train(X,y)\n",
        "\n",
        "  plt.plot(X,y)\n",
        "  plt.plot(X,model.predict(X))\n",
        "# TestRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8rywMwpP0d3L"
      },
      "outputs": [],
      "source": [
        "def TestClassification():\n",
        "  # Dataset code from https://cs231n.github.io/neural-networks-case-study/#data\n",
        "  N = 100 # number of points per class\n",
        "  D = 2 # dimensionality\n",
        "  K = 3 # number of classes\n",
        "  X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
        "  y = np.zeros(N*K, dtype='uint8') # class labels\n",
        "  for j in range(K):\n",
        "    ix = range(N*j,N*(j+1))\n",
        "    r = np.linspace(0.0,1,N) # radius\n",
        "    t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
        "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "    y[ix] = j\n",
        "  # lets visualize the data:\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "  plt.show()\n",
        "\n",
        "  optimizer = OptimizerAdam(learning_rate=0.01,n_epochs=10000,batch_size = 0,loss=LossCrossEntropy(),printEvery = 1000)\n",
        "\n",
        "  model = Sequential(optimizer)\n",
        "  model.add(Dense(2,100,activation = ActivationReLU()))\n",
        "  model.add(Dense(100,3,activation = ActivationSoftmax()))\n",
        "\n",
        "  model.train(X,y)\n",
        "\n",
        "  X_visualize = cartesian([np.arange(-1,1,0.01), np.arange(-1,1,0.01)])\n",
        "  y_pred = np.argmax(model.predict(X_visualize), axis=1)\n",
        "  plt.scatter(X_visualize[:, 0], X_visualize[:, 1], c=y_pred, s=40, cmap=plt.cm.gray,marker='.')\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral, marker='o')\n",
        "  plt.show()\n",
        "# TestClassification()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8dAPCORcHwV",
        "outputId": "064d0a72-3e9a-4279-8cba-f9963e2ba912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 12430 characters, 54 unique.\n"
          ]
        }
      ],
      "source": [
        "# some code here is taken from https://gist.github.com/karpathy/d4dee566867f8291f086\n",
        "# data = \"The quick brown fox jumps over the lazy dog. \"\n",
        "data = open('input.txt', 'r').read()\n",
        "chars = list(set(data))\n",
        "chars.sort()\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# seq_length = 10\n",
        "Xt = np.array([char_to_ix[ch] for ch in data[:-1]]).reshape((-1,1))\n",
        "yt = np.array([char_to_ix[ch] for ch in data[1:]]).reshape((-1,1))\n",
        "\n",
        "# print(''.join(ix_to_char[ix[0]] for ix in Xt))\n",
        "# print(''.join(ix_to_char[ix[0]] for ix in yt))\n",
        "\n",
        "X = np.zeros((data_size-1,vocab_size))\n",
        "for i in range(data_size-1):\n",
        "  X[i][Xt[i]] = 1\n",
        "\n",
        "# y = np.zeros((data_size-1,vocab_size))\n",
        "# for i in range(data_size-1):\n",
        "#   y[i][yt[i]] = 1\n",
        "\n",
        "y = yt.T[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lsJc6Lc4oo9-"
      },
      "outputs": [],
      "source": [
        "def showSample(model):\n",
        "  sample = np.copy(X[0][np.newaxis])\n",
        "  for i in range(100):\n",
        "    next = model.predict(sample)\n",
        "    sample = np.vstack((sample,next[-1,:]))\n",
        "\n",
        "  y_pred = np.argmax(sample, axis=1)\n",
        "  print(''.join(ix_to_char[ix] for ix in y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RfsdGclRjq3p"
      },
      "outputs": [],
      "source": [
        "def TestTextGeneration():\n",
        "  optimizer = OptimizerAdam(learning_rate=0.01,n_epochs=1000,batch_size = 64,loss=LossCrossEntropy(),displayProgress = showSample, printEvery=1)\n",
        "\n",
        "  hidden_size = 128\n",
        "\n",
        "  model = Sequential(optimizer)\n",
        "  model.add(LSTM(vocab_size,hidden_size,activation = ActivationLinear()))\n",
        "  # model.add(Recurrent(vocab_size,hidden_size,activation = ActivationTanh()))\n",
        "  # model.add(Recurrent(hidden_size,hidden_size,activation = ActivationTanh()))\n",
        "  # model.add(Dense(hidden_size,hidden_size,activation = ActivationTanh()))\n",
        "  model.add(Dense(hidden_size,vocab_size,activation = ActivationSoftmax()))\n",
        "\n",
        "  model.train(X,y)\n",
        "# TestTextGeneration()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HKb3pm1_JO_y"
      },
      "outputs": [],
      "source": [
        "def TestContinueTraining():\n",
        "  model = loadModel(\"model-1661265700/checkpoint-57-0.15962206.pickle\")\n",
        "\n",
        "  model.train(X,y)\n",
        "# TestContinueTraining()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4M7hPj8wdts-"
      },
      "outputs": [],
      "source": [
        "def samplePrompt(model,prompt,length):\n",
        "  Xt = np.array([char_to_ix[ch] for ch in prompt]).reshape((-1,1))\n",
        "\n",
        "  X = np.zeros((len(prompt),vocab_size))\n",
        "  for i in range(len(prompt)):\n",
        "    X[i][Xt[i]] = 1\n",
        "\n",
        "  sample = np.copy(X)\n",
        "  for i in range(length):\n",
        "    next = model.predict(sample)\n",
        "    sample = np.vstack((sample,next[-1,:]))\n",
        "\n",
        "  y_pred = np.argmax(sample, axis=1)\n",
        "  print(''.join(ix_to_char[ix] for ix in y_pred))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HriMk67od4BA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aad5e61-578d-4138-c434-88e78546dd4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am writing this under an appreciable mental strain, sharmly id meriniul.\n",
            "The end is near. I hear a noise at the door, as of some immense slippery body l\n"
          ]
        }
      ],
      "source": [
        "def TestSampleModel():\n",
        "  model = loadModel(\"model-1661265700/checkpoint-57-0.15962206.pickle\")\n",
        "  samplePrompt(model,\"I am writing this under an appreciable mental strain, \",100)\n",
        "TestSampleModel()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}